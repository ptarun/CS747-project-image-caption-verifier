{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nimport os\nimport tensorflow as tf\n# Download caption annotation files\nannotation_folder = '/annotations/'\nif not os.path.exists(os.path.abspath('.') + annotation_folder):\n  annotation_zip = tf.keras.utils.get_file('captions.zip',\n                                           cache_subdir=os.path.abspath('.'),\n                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n                                           extract=True)\n  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n  os.remove(annotation_zip)\n\n# Download image files\nimage_folder = '/train2014/'\nif not os.path.exists(os.path.abspath('.') + image_folder):\n  image_zip = tf.keras.utils.get_file('train2014.zip',\n                                      cache_subdir=os.path.abspath('.'),\n                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n                                      extract=True)\n  PATH = os.path.dirname(image_zip) + image_folder\n  os.remove(image_zip)\nelse:\n  PATH = os.path.abspath('.') + image_folder\n'''\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T17:11:12.102648Z","iopub.execute_input":"2022-04-29T17:11:12.103419Z","iopub.status.idle":"2022-04-29T17:11:12.109812Z","shell.execute_reply.started":"2022-04-29T17:11:12.103377Z","shell.execute_reply":"2022-04-29T17:11:12.108903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom pycocotools.coco import COCO\n\ndataDir='../input/coco-2017-dataset/coco2017/'\ndataType='train2017'\nannFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\ncoco=COCO(annFile)\n\ncapFile='{}/annotations/captions_{}.json'.format(dataDir,dataType)\ncoco_caps = COCO(capFile)\nids = list(coco.anns.keys())","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:51:57.163518Z","iopub.execute_input":"2022-04-30T05:51:57.164350Z","iopub.status.idle":"2022-04-30T05:53:23.885484Z","shell.execute_reply.started":"2022-04-30T05:51:57.164304Z","shell.execute_reply":"2022-04-30T05:53:23.884463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata_dict = {}\nfor item in coco.dataset['images']:\n        data_dict[item['id']] = item['file_name'] \n\ncaps_list = list(coco_caps.anns.values())\n\ncap_dict = {}\nfor item in caps_list:\n        try:\n            cap_dict[item['image_id']].append(item['caption'])\n        except:\n            cap_dict[item['image_id']] = [item['caption']]\n    \ncap_df = pd.DataFrame(cap_dict.items(), columns=['image_id', 'caption'])\nimage_df = pd.DataFrame(data_dict.items(), columns=['image_id', 'file_name'])\ndata_df = pd.merge(cap_df, image_df, on=\"image_id\")\ndata_df = data_df.iloc[:6000, :]\ndata_df","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:23.888844Z","iopub.execute_input":"2022-04-30T05:53:23.889201Z","iopub.status.idle":"2022-04-30T05:53:25.014345Z","shell.execute_reply.started":"2022-04-30T05:53:23.889167Z","shell.execute_reply":"2022-04-30T05:53:25.013318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_df.index)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:25.015817Z","iopub.execute_input":"2022-04-30T05:53:25.016055Z","iopub.status.idle":"2022-04-30T05:53:25.022969Z","shell.execute_reply.started":"2022-04-30T05:53:25.016026Z","shell.execute_reply":"2022-04-30T05:53:25.022025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating dataframe from coco \nimport pandas as pd\n\ndef create_df_from_coco(coco, coco_caps):\n    \n    data_dict = {}\n    for item in coco.dataset['images']:\n        data_dict[item['id']] = item['file_name'] \n        \n        \n    caps_list = list(coco_caps.anns.values())  \n    cap_dict = {}\n    for item in caps_list:\n        try:\n            cap_dict[item['image_id']].append(item['caption'])\n        except:\n            cap_dict[item['image_id']] = [item['caption']]\n    \n    cap_df = pd.DataFrame(cap_dict.items(), columns=['image_id', 'caption'])\n    image_df = pd.DataFrame(data_dict.items(), columns=['image_id', 'file_name'])\n    data_df = pd.merge(cap_df, image_df, on=\"image_id\")\n    data_df = data_df = data_df.iloc[:6000, :]\n    \n    print(len(cap_df), len(image_df), len(data_df))\n    train_captions = []\n    img_name_vector = []\n    caption_labels = []\n    img_path_list = []\n    \n    \n\n    for ind in data_df.index:\n        image_path =  '../input/coco-2017-dataset/coco2017/train2017/' + data_df['file_name'][ind]\n        #print(data_df['caption'][ind], data_df['file_name'][ind])\n        caption_list = data_df['caption'][ind]\n        train_captions.extend(caption_list)\n        img_name_vector.extend([image_path] * len(caption_list))\n        caption_labels.extend([1 for i in range(len(caption_list))]) \n        \n        img_path_list.extend([image_path])\n        \n        \n    #get negative samples\n    for ind in data_df.index[4000:6000]:\n        image_path =  '../input/coco-2017-dataset/coco2017/train2017/' + data_df['file_name'][ind]\n        #print(data_df['caption'][ind], data_df['file_name'][ind])\n        random_set = np.random.randint(4000, size=5)\n        caption_list = [data_df['caption'][i][0] for i in random_set]\n       \n        train_captions.extend(caption_list)\n        img_name_vector.extend([image_path] * len(caption_list))\n        caption_labels.extend([0 for i in range(len(caption_list))]) \n        \n    train_data_df = pd.DataFrame(list(zip(img_name_vector, train_captions, caption_labels)),\n               columns =['Image', 'Caption', 'Label'])\n    \n    \n    \n    return train_data_df, img_path_list\n    \ntrain_data_df, img_path_list = create_df_from_coco(coco, coco_caps)\ntrain_data_df","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:25.025159Z","iopub.execute_input":"2022-04-30T05:53:25.025654Z","iopub.status.idle":"2022-04-30T05:53:28.586475Z","shell.execute_reply.started":"2022-04-30T05:53:25.025621Z","shell.execute_reply":"2022-04-30T05:53:28.583039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = set(img_path_list)\nlen(s)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:28.591055Z","iopub.execute_input":"2022-04-30T05:53:28.593423Z","iopub.status.idle":"2022-04-30T05:53:28.608917Z","shell.execute_reply.started":"2022-04-30T05:53:28.593332Z","shell.execute_reply":"2022-04-30T05:53:28.607689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n#print(train_data_df['Caption'][601749])\n#Image.open(train_data_df['Image'][601749])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:47.981056Z","iopub.execute_input":"2022-04-30T05:53:47.981857Z","iopub.status.idle":"2022-04-30T05:53:47.987798Z","shell.execute_reply.started":"2022-04-30T05:53:47.981803Z","shell.execute_reply":"2022-04-30T05:53:47.986798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ntrain_data_df['Caption'] = train_data_df['Caption'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\ntrain_data_df","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:50.343488Z","iopub.execute_input":"2022-04-30T05:53:50.343958Z","iopub.status.idle":"2022-04-30T05:53:50.508315Z","shell.execute_reply.started":"2022-04-30T05:53:50.343915Z","shell.execute_reply":"2022-04-30T05:53:50.507281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make all lower case\ntrain_data_df['Caption'] = train_data_df['Caption'].str.lower()\ntrain_data_df = train_data_df.reset_index(drop=True)\ntrain_data_df","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:53.677038Z","iopub.execute_input":"2022-04-30T05:53:53.677435Z","iopub.status.idle":"2022-04-30T05:53:53.725700Z","shell.execute_reply.started":"2022-04-30T05:53:53.677377Z","shell.execute_reply":"2022-04-30T05:53:53.724951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize\ntrain_data_df['Caption'] = train_data_df.apply(lambda row: word_tokenize(row['Caption']), axis=1)\n\ntrain_data_df","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:53:56.657499Z","iopub.execute_input":"2022-04-30T05:53:56.657947Z","iopub.status.idle":"2022-04-30T05:54:04.864376Z","shell.execute_reply.started":"2022-04-30T05:53:56.657915Z","shell.execute_reply":"2022-04-30T05:54:04.863299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_data_df\na = list(df['Caption'].str.len())\nprint('max :', max(a))\nprint('avg :', sum(a)/len(a))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:04.866061Z","iopub.execute_input":"2022-04-30T05:54:04.866325Z","iopub.status.idle":"2022-04-30T05:54:04.912992Z","shell.execute_reply.started":"2022-04-30T05:54:04.866294Z","shell.execute_reply":"2022-04-30T05:54:04.912331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.displot(a , color=\"green\", bins=20)\nplt.xlabel('Len of caption', fontsize=16)\nplt.xlim(0,40)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:04.914161Z","iopub.execute_input":"2022-04-30T05:54:04.914877Z","iopub.status.idle":"2022-04-30T05:54:05.628018Z","shell.execute_reply.started":"2022-04-30T05:54:04.914838Z","shell.execute_reply":"2022-04-30T05:54:05.626964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change the values later\nmax_c_len = 20\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:09.528992Z","iopub.execute_input":"2022-04-30T05:54:09.529532Z","iopub.status.idle":"2022-04-30T05:54:09.534894Z","shell.execute_reply.started":"2022-04-30T05:54:09.529498Z","shell.execute_reply":"2022-04-30T05:54:09.533132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(train_data_df['Caption'])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:11.324140Z","iopub.execute_input":"2022-04-30T05:54:11.324521Z","iopub.status.idle":"2022-04-30T05:54:11.767322Z","shell.execute_reply.started":"2022-04-30T05:54:11.324485Z","shell.execute_reply":"2022-04-30T05:54:11.766659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voc_size = len(x_tokenizer.word_index) + 1\nvoc_size","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:13.083490Z","iopub.execute_input":"2022-04-30T05:54:13.084041Z","iopub.status.idle":"2022-04-30T05:54:13.089640Z","shell.execute_reply.started":"2022-04-30T05:54:13.084005Z","shell.execute_reply":"2022-04-30T05:54:13.088843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################################testing\n\nf = open('../input/glove-embeddings/glove.6B.300d.txt', 'r')\nlines = f.readlines()\n\nfrom tqdm import tqdm\nimport numpy as np\nwords_list = []\nglove_emb = {}\nfor i, line in enumerate(lines):\n    line = line.strip('\\n')\n    tokens = line.split()\n    temp = list(map(float, tokens[1:]))\n    word = tokens[0].lower()\n    words_list.append(word)\n    glove_emb[word] = np.array(temp)\n    #print(len(temp))\n\n\nnum_tokens = len(x_tokenizer.word_index) + 1\nembedding_dim = 300\nhits = 0\nmisses = 0\nm = []\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in x_tokenizer.word_index.items():\n    try:\n        embedding_vector = glove_emb[word]\n    \n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    except:\n        misses += 1\n        m.append(word)\n        #print(word)\nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n\n################################testing ended\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:54:16.790077Z","iopub.execute_input":"2022-04-30T05:54:16.790354Z","iopub.status.idle":"2022-04-30T05:55:22.915383Z","shell.execute_reply.started":"2022-04-30T05:54:16.790325Z","shell.execute_reply":"2022-04-30T05:55:22.914234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntrain_emb_c = x_tokenizer.texts_to_sequences(train_data_df['Caption']) \n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:22.917306Z","iopub.execute_input":"2022-04-30T05:55:22.918224Z","iopub.status.idle":"2022-04-30T05:55:23.279432Z","shell.execute_reply.started":"2022-04-30T05:55:22.918180Z","shell.execute_reply":"2022-04-30T05:55:23.278371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making all instances same length\ntrain_emb_c = pad_sequences(train_emb_c,  maxlen=max_c_len, padding='post')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:23.281327Z","iopub.execute_input":"2022-04-30T05:55:23.281749Z","iopub.status.idle":"2022-04-30T05:55:23.473774Z","shell.execute_reply.started":"2022-04-30T05:55:23.281702Z","shell.execute_reply":"2022-04-30T05:55:23.472783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image classifieser\nfrom keras import backend as K \n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom string import digits\nimport re\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed,Attention\nfrom tensorflow.keras.models import Model,load_model, model_from_json\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pickle as pkl\nimport numpy as np\n\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Attention\nK.clear_session() \nlatent_dim = 300\n\n\n\n\n#context lstm\n \ncontext_inputs = Input(shape=(max_c_len,)) \ncontxt_emb = Embedding(voc_size, latent_dim, weights=[embedding_matrix],trainable=True)(context_inputs) \n#LSTM 1 \ncontext_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output1, context_state_h1, context_state_c1 = context_lstm1(contxt_emb) \n\n\n#LSTM 2 \ncontext_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output, context_state_h, context_state_c = context_lstm2(context_output1)\n\n'''\n#LSTM 3 \ncontext_lstm3 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output, context_state_h, context_state_c = context_lstm3(context_output2)\n'''\n\npredictions = Dense(1, activation=\"sigmoid\", name=\"predictions\")(context_output)\n\nmodel = Model(context_inputs, predictions) \nplot_model(model, to_file='train_model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:44:33.338914Z","iopub.execute_input":"2022-04-29T15:44:33.339215Z","iopub.status.idle":"2022-04-29T15:44:34.399429Z","shell.execute_reply.started":"2022-04-29T15:44:33.339184Z","shell.execute_reply":"2022-04-29T15:44:34.398329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:44:37.423436Z","iopub.execute_input":"2022-04-29T15:44:37.423768Z","iopub.status.idle":"2022-04-29T15:44:37.436198Z","shell.execute_reply.started":"2022-04-29T15:44:37.423733Z","shell.execute_reply":"2022-04-29T15:44:37.435305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( train_emb_c, train_data_df['Label'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:44:39.596258Z","iopub.execute_input":"2022-04-29T15:44:39.596880Z","iopub.status.idle":"2022-04-29T15:44:39.720224Z","shell.execute_reply.started":"2022-04-29T15:44:39.596832Z","shell.execute_reply":"2022-04-29T15:44:39.719343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Fit the model using the train and test datasets.\nmodel.fit(X_train,y_train, epochs=3)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:43:17.244168Z","iopub.execute_input":"2022-04-29T15:43:17.244475Z","iopub.status.idle":"2022-04-29T15:43:32.628885Z","shell.execute_reply.started":"2022-04-29T15:43:17.244419Z","shell.execute_reply":"2022-04-29T15:43:32.627882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n# InceptionV3 model trained on imagenet data\nmodel = InceptionV3(weights='imagenet')\n# Remove the last layer \nmodel_new = Model(model.input, model.layers[-2].output)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:23.476193Z","iopub.execute_input":"2022-04-30T05:55:23.476486Z","iopub.status.idle":"2022-04-30T05:55:31.519061Z","shell.execute_reply.started":"2022-04-30T05:55:23.476453Z","shell.execute_reply":"2022-04-30T05:55:31.517946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input \nfrom tensorflow.keras.preprocessing import image\n\ndef preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n \n    x = image.img_to_array(img)\n\n    x = np.expand_dims(x, axis=0)\n \n    x = preprocess_input(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:31.520372Z","iopub.execute_input":"2022-04-30T05:55:31.520646Z","iopub.status.idle":"2022-04-30T05:55:31.529375Z","shell.execute_reply.started":"2022-04-30T05:55:31.520616Z","shell.execute_reply":"2022-04-30T05:55:31.527852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to encode a given image into a vector of size (2048, )\ndef encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) \n    return fea_vec","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:31.531868Z","iopub.execute_input":"2022-04-30T05:55:31.532225Z","iopub.status.idle":"2022-04-30T05:55:31.544145Z","shell.execute_reply.started":"2022-04-30T05:55:31.532182Z","shell.execute_reply":"2022-04-30T05:55:31.543127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the funtion to encode all the train images\n# This will take a while on CPU - Execute this only once\nimg = '../input/coco2014/train2014/train2014/COCO_train2014_000000000009.jpg'\nencoding_train = {}\n\nencoding_train[img] = encode(img)\nencoding_train[img]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:31.547179Z","iopub.execute_input":"2022-04-30T05:55:31.547645Z","iopub.status.idle":"2022-04-30T05:55:34.923092Z","shell.execute_reply.started":"2022-04-30T05:55:31.547610Z","shell.execute_reply":"2022-04-30T05:55:34.922394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nencoding_train = {}\nfor img in tqdm(img_path_list):\n    encoding_train[img] = encode(img)\n    \n\n#arr = os.listdir('../input/coco2014/train2014/train2014/')\n#arr","metadata":{"execution":{"iopub.status.busy":"2022-04-30T05:55:34.924327Z","iopub.execute_input":"2022-04-30T05:55:34.924696Z","iopub.status.idle":"2022-04-30T06:18:25.781060Z","shell.execute_reply.started":"2022-04-30T05:55:34.924667Z","shell.execute_reply":"2022-04-30T06:18:25.779840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Save the bottleneck train features to disk\nwith open(\"encoded_images.pkl\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_train, encoded_pickle)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T06:22:00.169901Z","iopub.execute_input":"2022-04-30T06:22:00.170347Z","iopub.status.idle":"2022-04-30T06:22:00.367841Z","shell.execute_reply.started":"2022-04-30T06:22:00.170290Z","shell.execute_reply":"2022-04-30T06:22:00.366830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = pickle.load(open(\"encoded_images.pkl\", \"rb\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-30T06:22:48.866042Z","iopub.execute_input":"2022-04-30T06:22:48.866325Z","iopub.status.idle":"2022-04-30T06:22:48.946800Z","shell.execute_reply.started":"2022-04-30T06:22:48.866297Z","shell.execute_reply":"2022-04-30T06:22:48.945923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image classifieser\nfrom keras import backend as K \n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom string import digits\nimport re\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed,Attention, Dropout\nfrom tensorflow.keras.models import Model,load_model, model_from_json\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers.merge import add\nimport pickle as pkl\nimport numpy as np\n\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Attention\nK.clear_session() \nlatent_dim = 300\n\n\n\n\n#context lstm\n \ncontext_inputs = Input(shape=(max_c_len,)) \ncontxt_emb = Embedding(voc_size, latent_dim, weights=[embedding_matrix],trainable=True)(context_inputs) \n#LSTM 1 \ncontext_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output1, context_state_h1, context_state_c1 = context_lstm1(contxt_emb) \n\n\n#LSTM 2 \ncontext_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output, context_state_h, context_state_c = context_lstm2(context_output1)\n\n'''\n#LSTM 3 \ncontext_lstm3 = LSTM(latent_dim,return_sequences=True,return_state=True) \ncontext_output, context_state_h, context_state_c = context_lstm3(context_output2)\n'''\n\n# image embedding\nimage_inputs = Input(shape=(2048,))\nimage_dropout = Dropout(0.5)(image_inputs)\nimage_outputs = Dense(latent_dim, activation='relu')(image_dropout)\n\n\n\ndecoder_concat_input = add([context_output, image_outputs])\n\npredictions = Dense(1, activation=\"sigmoid\", name=\"predictions\")(decoder_concat_input)\n\nmodel = Model([context_inputs, image_inputs], predictions) \nplot_model(model, to_file='train_model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T06:29:16.052122Z","iopub.execute_input":"2022-04-30T06:29:16.052446Z","iopub.status.idle":"2022-04-30T06:29:18.087223Z","shell.execute_reply.started":"2022-04-30T06:29:16.052413Z","shell.execute_reply":"2022-04-30T06:29:18.086369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( train_emb_c, train_data_df['Label'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T06:29:31.667673Z","iopub.execute_input":"2022-04-30T06:29:31.668010Z","iopub.status.idle":"2022-04-30T06:29:31.704801Z","shell.execute_reply.started":"2022-04-30T06:29:31.667973Z","shell.execute_reply":"2022-04-30T06:29:31.703518Z"},"trusted":true},"execution_count":null,"outputs":[]}]}